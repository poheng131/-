import requests
import time
import random
import pandas as pd
from datetime import datetime 
import os  
import ast

#å…ˆF12é–‹å•Ÿé–‹ç™¼è€…æ¨¡å¼ï¼Œç¢ºèªåƒæ•¸
keywords="æ•¸æ“šåˆ†æ"
page=1

url="https://www.104.com.tw/jobs/search/api/jobs"

params={
    "jobsource":"m_joblist_search",
    "keyword":keywords,
    "mode":"s",
    "order":15,
    "page":page,
    "pagesize":20,
}

headers={
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
    "Referer":"https://www.104.com.tw/",
}


# âœ… ç™½åå–®ï¼šåªä¿ç•™é€™äº›è·ç¼ºåç¨±é—œéµå­—çš„è·ç¼º
WHITELIST_KEYWORDS = [
    "æ•¸æ“šåˆ†æ", "è³‡æ–™åˆ†æ", "data analyst", "data analysis", 
    "data analytic", "è³‡æ–™ç§‘å­¸", "data scientist",
    "è³‡æ–™å·¥ç¨‹", "data engineer", "å•†æ¥­åˆ†æ", "bi", 
    "biå·¥ç¨‹å¸«", "bi analyst", "powerbi", "business intelligence",
    "business analyst", "machine learning", "AIåˆ†æ"
]

# âŒ é»‘åå–®ï¼šæ’é™¤é€™äº›æ˜é¡¯ä¸ç›¸é—œçš„è·ç¼º
EXCLUDE_WORDS = ["åŠ©ç†", "å®¢æœ", "é–€å¸‚", "å„²å‚™å¹¹éƒ¨", "å·¥è®€", "è¬›å¸«", "ä½œæ¥­å“¡", "è¡Œæ”¿", "æ¥­å‹™", "å¤–åŒ…", "è¨­è¨ˆ"]

def is_relevant_job(title):
    title=title.lower()
    return any(good in title for good in WHITELIST_KEYWORDS) and not any(bad in title for bad in EXCLUDE_WORDS)

#---ç¬¬äºŒå±¤çˆ¬èŸ²---æ‰€éœ€åƒæ•¸èˆ‡å‡½æ•¸
detail_headers={
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
    "Referer":"", #å…ˆç•™ç©º
    "X-Requested-With": "XMLHttpRequest",
}
def fetch_job_detail(first_layer_data):
    link_data = first_layer_data["link"]
    print(f"Link data: {link_data}")  # ç¢ºèªlink_dataçš„åŸå§‹æ ¼å¼

    # å¦‚æœ link_data å·²ç¶“æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨å®ƒï¼›å¦‚æœæ˜¯å­—ä¸²ï¼Œå‰‡è§£æå®ƒ
    if isinstance(link_data, str):
        try:
            link_data = ast.literal_eval(link_data)
        except Exception as e:
            print(f"è§£æé€£çµè³‡æ–™éŒ¯èª¤: {e}")
            return None
    
    job_url = link_data["job"]
    job_no = job_url.strip("/").split("/")[-1]
    detail_url = f"https://www.104.com.tw/job/ajax/content/{job_no}"

    detail_headers_current = detail_headers.copy()
    detail_headers_current["Referer"] = link_data["job"]

    try:
        response = requests.get(url=detail_url, headers=detail_headers_current, timeout=15)
        response.raise_for_status()  # å¦‚æœstatusä¸æ˜¯200ï¼Œæœƒæ‹‹å‡ºHTTPError
        detail_data = response.json()
        print(f"Returned detail_data: {detail_data}")  # æ‰“å°è¿”å›çš„è³‡æ–™ï¼Œçœ‹çœ‹å®ƒçš„çµæ§‹

        if isinstance(detail_data["data"], list):  # æª¢æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨
            print("Returned 'data' is a list, processing first item...")
            return detail_data["data"][0]  # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€å€‹é …ç›®
        elif isinstance(detail_data["data"], dict):  # å¦‚æœæ˜¯å­—å…¸
            return detail_data["data"]
        else:
            print("Unexpected structure of 'data' field")
            return None
    except requests.exceptions.RequestException as e:
        print(f"è«‹æ±‚éŒ¯èª¤: {e}")
        print(f"å›æ‡‰å…§å®¹: {response.text}")  # æ‰“å°å‡ºå›æ‡‰çš„åŸå§‹æ–‡æœ¬
        return None
    except ValueError as e:
        print(f"è§£æJSONéŒ¯èª¤: {e}")
        print(f"å›æ‡‰å…§å®¹: {response.text}")  # æ‰“å°å‡ºå›æ‡‰çš„åŸå§‹æ–‡æœ¬
        return None



#é–‹å§‹è«‹æ±‚å›å‚³è³‡æ–™

all_data=[]  #å°‡è’é›†çš„è³‡æ–™å­˜åœ¨é€™å€‹ç©ºlistç•¶ä¸­
max_page=10
while page <= max_page:
    try:
        params["page"]=page
        print(f"ğŸ” æŠ“å–ã€Œ{keywords}ã€ç¬¬ {page} é ")
        response=requests.get(url=url,params=params,headers=headers)
        print(response.text)
        data=response.json()
        filter_jobs=[job for job in data["data"] if is_relevant_job(job["jobName"])] 
        detail_data_all=[]
        for job_item in filter_jobs:
            detail_info = fetch_job_detail(job_item) # ç²å–å–®ä¸€è·ä½çš„è©³ç´°è³‡æ–™
            if detail_info:
                merged_record = {**job_item, **detail_info} # å°‡å…©å€‹å­—å…¸åˆä½µ
                all_data.append(merged_record) # åŠ å…¥ç¸½åˆ—è¡¨
            else:
                all_data.append(job_item) # è‹¥ç„¡è©³ç´°è³‡æ–™ï¼Œå‰‡åªåŠ å…¥ç¬¬ä¸€å±¤æ•¸æ“š
        time.sleep(random.uniform(1.5,3.5)) #æ¨¡æ“¬äººæ“ä½œï¼Œéš¨æ©Ÿæ™‚é–“é–“éš”æ›é 
        page+=1
    except Exception as e:
        print(e)
        print("éŒ¯èª¤ï¼Œè§£æjsonå¤±æ•—!!")
        break

#å°‡å›å‚³å…§å®¹å„²å­˜æˆæª”æ¡ˆ
df=pd.DataFrame(all_data)  
today=datetime.today().strftime("%Y-%m-%d")
filename=f"104_rawdata_{today}.csv"
folder=r"C:\Users\FM_pc\Desktop\jobs_analysis_project\data"
os.makedirs(folder,exist_ok=True)  #ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨ï¼Œç„¡å‰‡å»ºç«‹
df.to_csv(os.path.join(folder,filename),index=False,encoding="utf-8-sig",quoting=1)
print(f"çˆ¬èŸ²å®Œæˆï¼Œå„²å­˜æˆ{filename}")
